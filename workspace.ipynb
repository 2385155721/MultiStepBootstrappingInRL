{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "from operator import itemgetter\n",
    "from mdp_matrix import GridWorld, WindyGridCliffMazeWorld, StochasticGridWorld\n",
    "from sarsa import sarsa\n",
    "from expected_sarsa import expected_sarsa\n",
    "from double_sarsa import double_sarsa\n",
    "from double_expected_sarsa import double_expected_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_state = [0, 0]\n",
    "\n",
    "test_rewards = [[i, j, -1] for i in range(5) for j in range(5)]\n",
    "test_rewards[2] = [0, 2, 1]\n",
    "test_rewards[23] = [4,3, 1]\n",
    "\n",
    "gw = GridWorld(5, test_rewards, terminal_states=[2, 23] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:15: DeprecationWarning: This function is deprecated. Please call randint(0, 3 + 1) instead\n"
     ]
    }
   ],
   "source": [
    "n_step_sarsa(gw, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "    \n",
    "def behaviour_policy(Q, s, A, epsilon=.1):\n",
    "    \"\"\"\n",
    "    Recall that off-policy learning is learning the value function for\n",
    "    one policy, \\pi, while following another policy, \\mu. Often, \\pi is\n",
    "    the greedy policy for the current action-value-function estimate, \n",
    "    and \\mu is a more exploratory policy, perhaps \\epsilon-greedy.\n",
    "    In order to use the data from \\pi we must take into account the \n",
    "    difference between the two policies, using their relative\n",
    "    probability of taking the actions that were taken.\n",
    "    \"\"\"\n",
    "    if np.random.random_sample() <= epsilon:\n",
    "        a = np.random.random_integers(0, A-1)\n",
    "    else:\n",
    "        a = np.argmax(Q[s][:])\n",
    "    return a\n",
    "\n",
    "def target_policy(Q, s):\n",
    "    return np.argmax(Q[s][:])\n",
    "\n",
    "def n_step_sarsa(mdp, max_episode, alpha = 0.1, gamma = 0.9, epsilon = 0.1, n = 10):\n",
    "    # Initialization\n",
    "    Q = [[0 for i in range(mdp.A)] for j in range(mdp.S)]\n",
    "    old_Q = Q\n",
    "    n_episode = 0\n",
    "    rewards_per_episode = []\n",
    "    Q_variances = []\n",
    "    max_reward = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while n_episode < max_episode:\n",
    "        # If there's no starting state, just start at state 0\n",
    "        try:\n",
    "            s = mdp.initial_state # Initialize s, starting state\n",
    "        except AttributeError:\n",
    "            s = 0\n",
    "\n",
    "        # initializations\n",
    "        T = sys.maxint\n",
    "        tau = 0\n",
    "        t = -1\n",
    "        stored_actions = {}\n",
    "        stored_rewards = {}\n",
    "        stored_states = {}\n",
    "\n",
    "        # With prob epsilon, pick a random action\n",
    "            \n",
    "        stored_actions[0] = behaviour_policy(Q, s, mdp.A)\n",
    "        reward_for_episode = 0\n",
    "        \n",
    "        while tau < (T-1):\n",
    "            t += 1\n",
    "            if t < T:\n",
    "                # take action A_t\n",
    "                \n",
    "                # Observe and store the next reward R_{t+1} and next state S_{t+1}\n",
    "                st1 = np.random.choice(range(mdp.S), p = mdp.T[s, stored_actions[t], :])\n",
    "                rt1 = mdp.R[st1]\n",
    "                \n",
    "                stored_rewards[t] = rt1\n",
    "                stored_states[t] = st1\n",
    "                \n",
    "                # TODO: is this the right place to put this?\n",
    "                total_reward += rt1\n",
    "                reward_for_episode += rt1\n",
    "                \n",
    "                # if s_{t+1} terminal\n",
    "                if mdp.is_terminal(st1):\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    stored_actions[t+1] = behaviour_policy(Q, s, mdp.A)\n",
    "            \n",
    "            tau = t - n\n",
    "            if tau >= 0:\n",
    "                # product from i = tau+1 to min(tau+n-1, T-1) \\pi(A_i | S_i) / \\mu(A_i|S_i)\n",
    "                rho = np.prod([(stored_actions[k] == behaviour_policy(Q, stored_states[k], mdp.A)) / ((stored_actions[k] != np.argmax(Q[stored_states[k]][:]))*epsilon*1/mdp.A + (stored_actions[k] == np.argmax(Q[stored_states[k]][:]))*(epsilon*1/mdp.A + (1-epsilon)))  for k in range(tau+1, min(tau+n-1, T-1)+1)])\n",
    "                \n",
    "                G = np.sum([gamma**(i-tau-1) * stored_rewards[i] for i in range(tau+1, min(tau+n, T)+1)])\n",
    "                \n",
    "                if tau + n < T:\n",
    "                    G = G + gamma**n * Q[stored_states[tau+n]][stored_actions[tau+n]]\n",
    "                s_tau = stored_states[tau]\n",
    "                a_tau = stored_actions[tau]\n",
    "                Q[s_tau][a_tau] += alpha * rho * (G - Q[s_tau][a_tau])\n",
    "                \n",
    "                print tau\n",
    "                print T\n",
    "                # if pi is being learned, ensure that pi(.|S_tau) is \\epsilon-greedy wrt Q\n",
    "          \n",
    "\n",
    "\n",
    "        if reward_for_episode > max_reward:\n",
    "            max_reward = reward_for_episode\n",
    "\n",
    "        rewards_per_episode.append(reward_for_episode)\n",
    "        Q_variances.append(np.var(Q))\n",
    "        \n",
    "        #TODO: should we instead do an on-policy run here to calculate the\n",
    "        # average reward for the episode?\n",
    "\n",
    "        n_episode += 1\n",
    "        print n_episode\n",
    "    return Q, total_reward/max_episode, max_reward, rewards_per_episode, Q_variances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
