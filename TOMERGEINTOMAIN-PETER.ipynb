{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Brief Theoretical Background\n",
    "\n",
    "## Off-Policy vs. On-Policy\n",
    "\n",
    "We begin with analyzing the variance in off-policy methods vs. on-policy methods. From [1], we know that for off-policy learning, importance sampling \"enables off-policy learning, but at the cost of increasing the variance of the updates.\" While weighted importance sampling [2] or adapative importance sampling (based on previous timestep variance)[3,6] can bring down this variance, typical n-step off-policy SARSA does not use this variance reduction technique as described in [1].\n",
    "\n",
    "TODO: can we get some equations up in here to prove that importance sampling off-policy is indeed more variant than on-policy SARSA\"\n",
    "\n",
    "### Getting Rid of Importance Sampling (n-step Tree Backup)\n",
    "\n",
    "In [4], where n-step Tree Backup is first introduced (without importance sampling in Section 4 of [4]), the authors bypass importance sampling by using backups from possible actions at each step combined with the probability of the target policy taking this action. This removes importance sampling and theoretically the variance introduced by it. Intuitively, this is the same bias-variance tradeoff as in Expected Sarsa. By using the expectation at each timestep we are bounding the variance (see [5] for more details on bias-variance tradeoff of Sarsa vs. Expected SARSA). \n",
    "\n",
    "### Combining Importance Sampling with Expecation in Tree Backup (i.e. n-step $Q(\\sigma)$)\n",
    "\n",
    "By adding function which balances sampling with the expectation backups of n-step tree backup, we can balance the variance. One can think of an easy extension such that the $\\sigma$ provided to the algorithm is a function of the previous step's variance (similar to adaptive importance sampling or autostep [3,6]). However, say that we simply choose the $\\sigma$ value to be a uniformly distributed random variable. TODO: maths here???\n",
    "\n",
    "## Effect of n on variance??\n",
    "\n",
    "TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] Precup, Doina, Richard S. Sutton, and Sanjoy Dasgupta. \"Off-policy temporal-difference learning with function approximation.\" ICML. 2001.\n",
    "[3] Hachiya, Hirotaka, et al. \"Adaptive importance sampling for value function approximation in off-policy reinforcement learning.\" Neural Networks 22.10 (2009): 1399-1410.\n",
    "[4] Precup, Doina. \"Eligibility traces for off-policy policy evaluation.\" Computer Science Department Faculty Publication Series (2000): 80.\n",
    "[5] Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on. IEEE, 2009.\n",
    "[6] Mahmood, Ashique Rupam, et al. \"Tuning-free step-size adaptation.\" Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
